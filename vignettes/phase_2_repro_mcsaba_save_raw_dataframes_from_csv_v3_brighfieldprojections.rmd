---
title: "Save raw single cell data for experiment"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
```

# Loading packages

```{r loading libraries}
library(tidyverse)
library(reticulate)
library(lubridate)

#clustering
library(umap)
# library(dbscan)
library(mclust)

#plotting
library(patchwork)
library(janitor)
library(platetools)
library(viridis)
library(purrr)

# database
# library(RPostgres)
# library(pool)
# library(RSQLite)
# library(DBI)

library(here)
library(googlesheets4)

library(dcphelper)
```

# Data access

## Setting data paths

```{r fetch selected measurements from database}

selected_barcode = "000012106403"

result_path = "~/dcp_helper/data/results/"
channel_of_interest = "ch1"
measurement_of_interest_ch3_ch4 = "measurement_ch3_ch4_Cells.csv"


measurement_ids = c("000012106403__2020-06-30T21_30_28-Measurement_1", "000012106403__2020-07-01T19_04_47-Measurement_2", "000012106403__2020-07-02T18_02_12-Measurement_3")

```

## Loading measurements

```{r}
measurement_list = list()

for (measurement_id in measurement_ids){
  
  files <- list.files(paste0(result_path, measurement_id), pattern = channel_of_interest, full.names = TRUE) %>%
    compact(.) %>%
    paste0(., "/", measurement_of_interest_ch3_ch4)
  print(list.files(paste0(result_path, measurement_id), pattern = channel_of_interest, full.names = TRUE) %>%
    compact(.) )

  measurement_list[[measurement_id]] <- tibble(
    id_barcode = measurement_id %>% str_extract(pattern = "0000\\d+"),
    id_measurement = measurement_id,
    id_observation = files %>% str_extract(pattern = "0000\\d+__\\d+-\\d\\d-\\d+T\\d+_\\d+_\\d+-Measurement_\\d-sk\\d+-...-f..-ch\\d")
         ) %>%
    separate(id_observation, remove = FALSE, sep = "-", c("t1", "t2", "t3",  "measurement_no", "iteration_no", "well", "field", "channel")) %>% select(-(t1:t3)) %>%
    mutate(measurement_no = str_extract(measurement_no, pattern = "\\d") %>% as.numeric(),
           iteration_no = str_extract(iteration_no, pattern = "\\d") %>% as.numeric()) %>%
    mutate(full_path = files,
           id_observation_checksum = tools::md5sum(full_path) %>% as.character()) 
}

measurement <- do.call("rbind", measurement_list)

rownames(measurement) <- c()

# measurement %>%
#   write_rds(here::here("data/measurement.Rds"))
```

## Loading metadata

```{r}
metadata_list = list()
for (measurement_id in measurement_ids){
  new_path_base = paste0("~/dcp_helper/metadata/", measurement_id,"/") #relative path acceptable
  inbox_path_base= paste0("/home/ubuntu/bucket/inbox/", measurement_id,"/Images/") #absolute path with /home/ubuntu/ required
  
  metadata_list[[measurement_id]] <- extract_filelist(path = inbox_path_base, force=FALSE, new_path_base) %>%
    reformat_filelist()
}

metadata <- do.call("rbind", metadata_list)
rownames(metadata) <- c()

metadata_time_annotation <- metadata %>%
  group_by(Metadata_timepoint, Metadata_well, Metadata_fld) %>%
  summarise(abstime = min(Metadata_abstime)) %>%
  mutate(Metadata_timepoint = as.numeric(substring(Metadata_timepoint,first=3,last=1000))) %>% 
  rename(iteration_no = Metadata_timepoint, well = Metadata_well, field = Metadata_fld, date = abstime)

measurement <- measurement %>%
  left_join(metadata_time_annotation, by=c("iteration_no","well","field"))

```

## Layout/treatment annotation

```{r reading plate annotation, eval=FALSE, message=FALSE, include=FALSE}
# Reading the compound submission file but ignoring the manual ID
plate_anno <- read_delim(here::here("data/annotation/20181016105007_registered_7296_NRindtorff_registration_Kekule.txt"), "\t", escape_double = FALSE, trim_ws = TRUE) %>% 
  # Using the Broad ID
  select(broad_sample = Broad_external_Id, compound_name) %>% 
  # Reading the annotation of the source plate for DMSO 
  left_join(read_delim(here::here("data/annotation/S-C-7296-01-B40-002.txt"), 
      "\t", escape_double = FALSE, trim_ws = TRUE) %>% select(well_position, 
                                                              broad_sample, 
                                                              mmoles_per_liter) %>% 
        drop_na() %>% 
        # Reading the annotation of the source plate H20
        rbind(.,read_delim(here::here("data/annotation/S-C_7296_01_B40_003.txt"), 
      "\t", escape_double = FALSE, trim_ws = TRUE) %>% select(well_position, 
                                                              broad_sample, 
                                                              mmoles_per_liter) %>% drop_na()), ., 
      # Joining by broad_id
      by = "broad_sample") %>%
  
  # Reading the annotation of the robot worklist that links source plate to destination plate
  left_join(read_csv(here::here("data/annotation/cclf_ascites_1910_complete_worklist.csv")) %>% 
    select(destination_well = well, well_position = source_well)) %>% 
  # Adding 0.1% DMSO wells
  select(-well_position, well = destination_well) %>%
  left_join(tibble(well = platetools::num_to_well(1:384, plate = 384)), .) %>% 
  mutate(broad_sample = if_else(is.na(broad_sample), "DMSO", broad_sample), 
         compound_name = if_else(is.na(compound_name), "DMSO", compound_name),
         mmoles_per_liter = if_else(is.na(mmoles_per_liter), 0, mmoles_per_liter)) %>% 
  mutate(compound_name = if_else(compound_name == "Boretzomib", "Bortezomib", compound_name),
         compound_name = if_else(compound_name == "Pembrolizumab", "H20/Pembrolizumab", compound_name)) %>%
  # I add section information
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS),
         col = substr(well, 2, 3) %>% as.numeric()) %>%
  mutate(section = case_when(col %in% c(1:4, 13:16) ~ 1,
                             col %in% c(5:8, 17:20) ~ 2,
                             col %in% c(9:12, 21:24) ~ 3))

# I write the library annotation file. 
write_csv(plate_anno, here::here("data/annotation/cclf_ascites_20200630_human_readable.csv"))
```

```{r, message=FALSE}
# Reading the compound submission file but ignoring the manual ID
# plate_anno <- read_sheet("https://docs.google.com/spreadsheets/d/1m_ZIterCCciUmneBVroE2LPdI_oLFmXeaURxGB-Jxjc/edit?usp=sharing")
gs4_deauth()
plate_anno <- read_sheet("1m_ZIterCCciUmneBVroE2LPdI_oLFmXeaURxGB-Jxjc")
# plate_anno <- read_delim(here::here("data/annotation/000012106403_layout.tsv"), "\t", escape_double = FALSE, trim_ws = TRUE)
names(plate_anno) <- tolower(names(plate_anno))

plate_anno <- plate_anno %>%
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS)) %>%
  mutate(well = paste0(substring(well,1,1), str_pad(substring(well,2,4), 2, "left", "0")))

# I write the library annotation file. 
write_csv(plate_anno, here::here("data/annotation/cclf_ascites_20200630_human_readable.csv"))
```

I plot the complete library. 

```{r, message=FALSE}
# plate_anno %>%
raw_map(data = plate_anno$tmrm,
        well = plate_anno$well,
        plate = 384) +
  ggtitle("cclf_ascites_TMRM") +
  scale_fill_brewer(type = "qual", palette = 2) +
  theme_dark() +
  ggsave(here::here("figures/plate_layout_tmrm.png"))

raw_map(data = plate_anno$cellevent_caspase,
        well = plate_anno$well,
        plate = 384) +
  scale_fill_brewer(type="qual")+
  ggtitle("cclf_ascites_CellEvent_Caspase")+
  theme_dark()+
  ggsave(here::here("figures/plate_layout_cellevent_caspase.png"))

raw_map(data = as.character(plate_anno$supplement),
        well = plate_anno$well,
        plate = 384) +
    scale_fill_brewer(type = "qual", palette = 2) +
    ggtitle("cclf_ascites_supplement")+
    theme_dark()+
  ggsave(here::here("figures/plate_layout_supplement.png"))

raw_map(data = as.character(plate_anno$cd45_depletion),
        well = plate_anno$well,
        plate = 384) +
    scale_fill_brewer(type = "qual", palette = 2) +
    ggtitle("cclf_ascites_cd45depletion")+
    theme_dark()+
  ggsave(here::here("figures/plate_layout_cd45depletion.png"))
```

## Loading observations

```{r, message=FALSE}
# print(paste0("Adding ", nrow(new_measurement), " new Measurements"))
#   new_measurement %>%
#     dbWriteTable(pool, "measurement", ., append = TRUE)

print("Writing observations")
# observation <- measurement %>%
#   unite("observation", contains("observation"), sep = "___") %>%
#   mutate(data = furrr::future_map2(observation, full_path, ~ {read_csv(.y) %>%
#                                      janitor::clean_names() %>%
#                                      mutate(observation = .x) %>%
#                                      separate(observation, c("id_observation", "id_observation_checksum"), sep = "___")}))

observation2 <- lapply(measurement, read.delim)
read_csv_with_metadata <- function(metadata_row){
  read_csv(metadata_row$full_path) %>%
    cbind(metadata_row)
}
# # 
# observation3 <- measurement %>%
#   unite("observation", contains("observation"), sep = "___") %>%
#   # rowwise() %>%
#   # apply(., 1, read_csv_with_metadata)
#   rowwise() %>%
#   read_csv_with_metadata(.)
#   
# # %>% dbWriteTable(pool, "observation", ., append = TRUE)
# #                   print(paste0("appended ", .y))}
#                                        ))
```



# Data Filtering

I extract the starting time for each plate using the exact timestamp. 

```{r, eval=FALSE}
measurement <- measurement %>% 
  # I drop NA entries, they are corrupted
  drop_na() %>%
  # mutate(measurement_no = if_else(id_barcode == "000012048703" & measurement_no == 4, 2, measurement_no)) %>% 
  # mutate(measurement_no = if_else(id_barcode == "000012048703" & measurement_no == 6, 3, measurement_no)) %>% 
  # extracting date
  mutate(date = str_extract(string = id_measurement, pattern = "20\\d\\d-\\d\\d-\\d\\dT\\d\\d_\\d\\d_\\d\\d") %>% 
           str_replace(pattern = "T", replacement = "-") %>% lubridate::ymd_hms())
  
```

Not every sample has 3 measurments on subsequent days. It will be hard to compare them. For now I build the analysis in a way that is only focused on the wall-clock imaging time.

```{r}
measurement %>% 
  select(id_barcode, date, measurement_no) %>% distinct() %>% arrange(date)
```

I fix some measurment_no indicators.

# Time imputations

I impute time based on the following assumptions: 
* the measurement timepoint in the barcode reflects the time when a measurement was started
* there is a constant time that is needed for well completion
* each measurement starts at well P01 and ends at well A24
* in some cases, measurement protocols have implemented a waiting time between each imaging round (multiple imaging rounds per measurement with a t>0s gap between the rounds. I don't think this is a good idea, but it has been done.) 

I manually annotate the imaging protocols for each plate in our dataset.

```{r, message=FALSE, eval=FALSE}
barcode_anno <- tibble(id_barcode = barcodes[1:3],
       library = "XXXX_2018/11/06",
       protocol = "ascites_mush_40x_mito_fitc_singlecell_z-2_lowerexp_fullplate_6h",
       confocal = FALSE,
       pause = FALSE) %>% # pause indicates wether or not the microscope took a forced break during acquisition. I don't think this is a good thing, but we introduced it when the confocal protocol turned out to be faster than others. We will get rid of it in the future. 
rbind(tibble(id_barcode = barcodes[4:6],
       library = "9375_2019/04/03",
       protocol = "ascites_mush_40x_mito_fitc_singlecell_z-2_lowerexp_fullplate_6h_with_break",
       confocal = TRUE,
       pause = TRUE)) %>% 
  cbind(sample = c("CCLF_cRCRF1066T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1047T",
                 "CCLF_cRCRF1050T",
                 "CCLF_cRCRF1068T")) %>% 
  left_join(read_csv(here::here("data/CCLF Ascites Samples - biosensor_cohort.csv")) %>% 
  select(sample, patient_id))
```

Read loaddata_output.csv files with imaging metadata

```{r, message=FALSE}
# well_timestamps <- read_csv('~/bucket/dcp_helper/metadata/000012106403__2020-06-30T21_30_28-Measurement_1/loaddata_output.csv') %>%
#   select(Metadata_Well, Metadata_AbsTime) %>%
#   group_by(Metadata_Well) %>%
#   summarise(time = min(Metadata_AbsTime)) %>%
#   rename( well = Metadata_Well )

build_loaddata_path <- function(measurement_id){ 
  paste0('~/bucket/dcp_helper/metadata/', measurement_id, '/loaddata_output.csv') 
}

loaddata_csv_list <- sapply(unique(measurement$id_measurement), build_loaddata_path)
measurement_ids <- names(loaddata_csv_list)

well_timestamps <- do.call(rbind, lapply(names(loaddata_csv_list), function(x) cbind(read_csv(loaddata_csv_list[[x]]), id_measurement=x))) %>%
  select(Metadata_Well, Metadata_AbsTime, Metadata_TimepointID, id_measurement) %>%
  mutate( Metadata_TimepointID = Metadata_TimepointID + 1) %>%
  # arrange(Metadata_AbsTime)
  group_by(id_measurement, Metadata_TimepointID,Metadata_Well) %>%
  summarise(time = min(Metadata_AbsTime)) %>%
  rename( well = Metadata_Well, iteration_no = Metadata_TimepointID ) %>%
  arrange(time)
```

I now create a manual well-wise annotation file for every barcode & measurement using the plate annotation as the basis.

```{r}
anno <- measurement %>% 
  select(contains("id"), well, date, contains("_no")) %>% 
  select(-contains("observation")) %>% 
  distinct() %>% 
  left_join(plate_anno, by  = "well") %>% 
  # left_join(barcode_anno) %>%
    mutate(row = substr(well, 1, 1) %>% match(., LETTERS),
         col = substr(well, 2, 3) %>% as.numeric())
  # joining anno file and formatting annotation according to the written/online documentation
  # mutate(content = case_when(id_barcode == "000012048703" & section == 1 ~ "CD45-",
  #                           id_barcode == "000012048703" & section == 2 ~ "raw",
  #                           id_barcode == "000012048703" & section == 2 ~ "empty",
  #                           # next plate
  #                           id_barcode == "000012048903" & section %in% c(1,2) ~ "CD45-",
  #                           id_barcode == "000012048903" & section == 3 ~ "CD45-_raw_1:1",
  #                           # next plate
  #                           id_barcode == "000012049003" & section %in% c(1,2) ~ "CD45-",
  #                           id_barcode == "000012049003" & section == 3 ~ "CD45-_raw_1:1",
  #                           # next plate, processing Mushriq's - need confirmation
  #                           id_barcode == "000012094903" & section %in% c(1,2) ~ "CD45-",
  #                           id_barcode == "000012094903" & section == 3 ~ "empty",
  #                           # next plate
  #                           id_barcode == "000012095103" & section %in% c(1,2) ~ "CD45-",
  #                           id_barcode == "000012095103" & section == 3 ~ "empty",
  #                           # next plate - needs confirmation by Mushriq
  #                           id_barcode == "000012095203" & section %in% c(1,2, 3) ~ "CD45-"),
  #         flag = case_when(id_barcode == "000012048703" & well %in% c("K16", "M16", "N13", "N14", 
                                                                     # "N15", "N16", "P13", "P14", "P15", "P16") ~ TRUE,
                          
                          # TRUE ~ FALSE)) %>% 
  # changing the library H20/Pembolizumab acc. to the library status. 
  # mutate(compound_name = if_else(compound_name == "H20/Pembrolizumab", "H20", compound_name)) %>%
  
  # Now I also add the imaging times. What matters, is the time per well that is spend per protocol. We can estimate that number based on the hand-written protocols and logs from the Opera microscope.
  # mutate(time_per_plate = case_when(confocal == FALSE ~ hms("6:37:23") %>% lubridate::seconds() %>% as.numeric(),
  #                                   confocal == TRUE ~ hms("4:49:23") %>% lubridate::seconds() %>% as.numeric()),
  #        time_per_well = time_per_plate/384)
```

Now I estimate the exact imaging time for each well based on the *time_per_well* feature and the device-specific order of imaging.
First, I define the a layout of the device-specific imaging order.

```{r}
# I define a layout of the device-specific order of imaging.
measurement_layout <- tibble(row = rep(c(LETTERS[c(16:1)], LETTERS[c(1:16)]), times = 12),
       col = rep(c(1:24), each = 16)) %>% 
  mutate(col = str_pad(col, 2, side = "left", pad = "0"),
         well = paste0(row, col),
         index = 1:384) %>% 
  dplyr::select(well, index)

# df <- measurement_layout
# 
# raw_map(data = df$index,
#         well = df$well,
#         plate = 384) +
#     ggtitle("Time layout for imaging protocol") +
#     theme_dark() +
#     scale_fill_viridis()
```

Now I combine the layout with the *time_per_well* estimates. 

```{r, eval=FALSE}
anno_w_time <- anno %>% 
  left_join(measurement_layout , by = "well") %>% 
  arrange(id_barcode, measurement_no, iteration_no, index) %>% 
  group_by(id_barcode, measurement_no) %>%
  mutate(time = cumsum(time_per_well)) %>% 
  arrange(id_barcode, measurement_no, iteration_no, index) %>% 
  # adding a the break time for some plates
  mutate(time = if_else(iteration_no == 2 & pause == TRUE, time + (6*3600-time_per_plate), time)) %>% 
  mutate(date = date + seconds(time)) %>%
  # overwritinig time to be the relative absolute number of hours since the very first well. 
  group_by(id_barcode) %>% 
  mutate(time = date - min(date),
         time = as.numeric(time),
         time = time/3600) # converting to hours.
```

```{r}
anno_w_time <- anno %>% 
  left_join(measurement_layout , by = "well") %>% 
  left_join(well_timestamps, by = c("id_measurement", "iteration_no", "well") ) %>%
  arrange(id_barcode, measurement_no, iteration_no, index) %>% 
  group_by(id_barcode, measurement_no) %>%
  # mutate(time = cumsum(time_per_well)) %>% 
  arrange(id_barcode, measurement_no, iteration_no, index) %>% 
  # adding a the break time for some plates
  # mutate(time = if_else(iteration_no == 2 & pause == TRUE, time + (6*3600-time_per_plate), time)) %>% 
  # mutate(date = date + seconds(time)) # %>%
  # overwritinig time to be the relative absolute number of hours since the very first well. 
  # group_by(id_barcode) %>% 
  mutate(ellapsed_time = time - min(time),
         ellapsed_time = as.numeric(ellapsed_time))
  #        time = as.numeric(time),
  #        time = time/3600) # converting to hours.
```

I can give a historic overview about the CCLF imaging activity.

```{r}
# anno_w_time %>% 
#   group_by(id_barcode, sample, library, patient_id) %>% 
#   summarise(date = min(date))  %>% 
#   ggplot(aes(date, sample, color = patient_id, label = id_barcode)) + 
#   geom_point(size = 3) + 
#   ggrepel::geom_text_repel(color = "black") +
#   theme_classic() + 
#   theme(legend.position = "bottom") + 
#   scale_color_brewer(type = "qual") + 
#   ggtitle("Samples and their time of acquisition") + 
#   labs(color = "Patient")
```


I plot the time layout for every plate. 

```{r, eval=FALSE}
df <- anno_w_time %>%
  unite(run, measurement_no, iteration_no, remove = FALSE) %>%
  select(run, id_barcode, measurement_no, well, ellapsed_time) %>%
  mutate(row = substr(well, 1, 1) %>% match(., LETTERS[26:1]),
         col = substr(well, 2, 3) %>% as.numeric()) %>%
  mutate(status = 1)

df %>%
  #filter(run %in% c("1_1", "1_2")) %>%
  ggplot(aes(col, row, fill = ellapsed_time)) +
  geom_tile() +
  facet_grid(run~id_barcode) +
  scale_fill_viridis_c() +
  ggtitle("Imaged wells during pilot phase, corrected") +
  theme_classic() +
  theme(axis.text.x = element_blank()) +
  theme(axis.text.y = element_blank(),
        legend.position = "bottom") +
  labs(fill = "Time in hours")
```

This approach to time inference has obvious weaknesses: 
* The method can not distinguish between wells that were purposefully omitted during image capture or failures during the image analysis workflow leading to missing data. While in the 1st scenario, the model is accurate, the second scenario leads to a systematic underestimation of actual acquisition time for every well thereafter.

I write the annotation to the database in a separate table.

```{r, eval = FALSE}
# set arguments carefully.
# DBI::dbWriteTable(pool, "annotation", anno_w_time, overwrite = FALSE, append = FALSE)
# annotation <- tbl(pool, "annotation")
```

# EDA and QC

The object segmentation is currently not ideal. Therefore I remove objects that are clearly over or under-segmented. I evaluate the size distribution of segmented objects and exclude objects that have extreme values. 

I filter all extremely sized objects from the database.

NOTE FOR CSABA: can be slow, can be skipped if data/percentiles.Rds exists

```{r, eval = FALSE}
# show behaviour of viability over time for untreated and control samples
# show robustness of sample viability behaviour over time between replicates
# show similar patterns in compound response for both biological replciates 
# start_time <- Sys.time()
# 
# percentiles <- dbGetQuery(pool, 'select
#   percentile_cont(0.01) within group (order by observation.area_shape_area),
#   percentile_cont(0.99) within group (order by observation.area_shape_area)
# from observation;')
# 
# percentiles_end_time <- Sys.time()
# print(end_time-start_time)
# 
# saveRDS(percentiles, here::here("data/percentiles.Rds"))
```

# Read individual CSV-s

```{r}
#obs <- dbGetQuery(pool, paste("SELECT * FROM observation WHERE id_barcode = '", barcodes[1], "'", sep=""))

start_time <- Sys.time()

meas <- measurement %>% filter(id_barcode == selected_barcode)

meas_time <- Sys.time()

obs <- meas %>% semi_join(observation %>% collect(), .)

obs_time <- Sys.time()

# obs %>% write_rds(here("data/observation.Rds"))

end_time <- Sys.time()

print(meas_time-start_time)
print(obs_time-start_time)
print(end_time-start_time)

```

```{r}
# print(barcodes[1])
# print(unique(obs$id_observation))
```


```{r}
# obs <- readRDS(here::here("data/observation.Rds"))
percentiles <- readRDS(here::here("data/percentiles.Rds"))
max_area <- max(percentiles[1,])
min_area <- min(percentiles[1,])
```

FOR CSABA AND NIKLAS TO CHECK: measurement table should include iteration_no before merging tables!?!?!?

```{r}

# updating the database
observation_f <- obs %>%
  filter(area_shape_area < max_area & area_shape_area > min_area) %>%
  left_join(measurement %>% select(contains('id'), well, iteration_no), copy = TRUE) %>%
  left_join(anno_w_time, copy = TRUE) #%>%
  # filter(flag == FALSE & content == "CD45-") %>%
  # filter(measurement_no < 4)

observation_f <- observation_f %>% 
  mutate(run= paste0(measurement_no, "_", iteration_no))

```

I am also creating a local realization based on these cutoff values. This is a subsample of the dataset.

```{r, eval = FALSE}
# observation_sample <- dbGetQuery(pool, 
# "SELECT * 
# FROM observation O
# WHERE O.id_observation IN (SELECT id_observation 
# FROM annotation A
# WHERE A.confocal = 'TRUE' AND A.compound_name = 'DMSO')
# ORDER BY RANDOM() 
# LIMIT 3000;")


# observation_f_real <- observation_f %>% 
#   filter((compound_name == 'DMSO' | compound_name == 'Bortezomib')) %>% 
#   # filter(confocal == TRUE) %>% 
#    collect()
# 
# observation_f_sample <- observation_f_real %>% 
#   select(contains("_ch3"), contains("_ch4"), 
#          #contains("area"),
#          id_measurement, id_barcode, time, compound_name) %>% 
#   group_by(id_measurement, compound_name) %>% 
#   sample_n(2000) %>%
#   ungroup() 
# 
# observation_f_umap_in <- observation_f_sample %>%
#   select(-contains("location"), -contains("_center_"), -id_measurement, -id_barcode,
#          -contains("number"), -time, -compound_name) 
# 
# observation_f_umap_out <- uwot::umap(observation_f_umap_in, pca = 50, verbose = TRUE) %>%
#   as_tibble() %>%
#   clean_names()
# 
# observation_f_umap_out %>%
#   cbind(observation_f_sample) %>%
#   mutate(ctrl = if_else(compound_name == "Bortezomib", TRUE, FALSE)) %>%
#  ggplot(aes(v1, v2, color = time)) + # area_shape_area intensity_integrated_intensity_ch3
#   geom_point(alpha = 0.1) + 
#   facet_grid(compound_name ~ id_barcode) +
#   scale_color_viridis_c() + 
#   theme_bw() + 
#   labs(title = "UMAP embedding of DMSO and Bortezomib treated cells across cohort",
#        x = "UMAP 1", 
#        y = "UMAP 2",
#        color = "Time [h]") + 
#   ggsave(here::here("umap_borte_dmso.png"), width = 7, height = 3.5)
  
```


```{r, eval = FALSE}
# gg_size_histogramm <- observation_f %>% 
#   filter(compound_name == "DMSO") %>% # A random DMSO treated well
#   select(area_shape_area, run, id_barcode, section) %>%
#   collect() %>%
#   ggplot(aes(area_shape_area)) + 
#   geom_histogram(alpha = 0.3, position="identity") + 
#   theme_classic() 
# 
# gg_size_histogramm + 
#   scale_fill_brewer(type = "qual") +
#   facet_grid(run~section) + 
#   labs(title = "Size Distribution of segmented objects",
#        subtitle = "after removing 1% and 99% percentile",
#        caption = "")
```

I am wondering if the number of objects stays the same over time and across wells. I calculate QC threshold values for each batch that remove wells, if the number of objects is unexpectedly large.

```{r, eval = FALSE}
# get_cell_count <- function(db){
#   cell_count <- list()
#   
#   cell_count$distribution <- db %>% 
#     count(well, run, id_barcode, compound_name) %>% 
#     collect()
#   
#   cell_count$threshold <- cell_count$distribution %>%
#     ungroup() %>%
#     group_by(run, id_barcode) %>% 
#     summarise(sd = sd(n),
#               mean = mean(n),
#               min = mean-3*sd,
#               max = mean+3*sd
#               )
#   
#   return(cell_count)
# }
# 
# cell_count <- observation_f %>% get_cell_count()
# 
# 
# cell_count$distribution %>%
#   ungroup() %>%
#   mutate(well = factor(well) %>% fct_inorder(),
#          n = as.numeric(n)) %>%
#   
#   ggplot(aes(well, n ))+
#   geom_point(aes(color = compound_name), alpha = 0.8) + 
#   
#   #geom_smooth(color = "red") +
#   geom_hline(data = cell_count$threshold, aes(yintercept = min), linetype = "dashed") + 
#   geom_hline(data = cell_count$threshold, aes(yintercept = max), linetype = "dashed") + 
#   
#   facet_grid(run ~ id_barcode) + 
#   
#   theme(axis.text.x = element_blank(),
#         legend.position = "right") + 
#   scale_color_brewer(type="qual", palette = 2) +
#   labs(title = "Number of objects and their distribution across the plate",
#        subtitle = "shown are well A01 to P24",
#        caption = "") +
#   ggsave(file = here::here(paste("figures/number_of_objects_distribution_barcode_", selected_barcode, ".png", sep="")))
```

```{r}

# cell_count_distribution <- cell_count$distribution %>%
#   ungroup() %>%
#   left_join(plate_anno %>%
#     select(well, section)) %>%
#   mutate(well = factor(well) %>% fct_inorder(),
#          n = as.numeric(n)) %>%
#   mutate(subsection = if_else(as.numeric(substring(well,2)) %in% c(1:12), 1, 2 ) )
# 
# 
# cell_count_distribution %>%
#   ggplot(aes(well, n )) + 
#   geom_point(aes(color = as.factor(section)), alpha = 0.8) + 
#   
#   #geom_smooth(aes(color = as.factor(section))) +
#   
#   geom_hline(data = cell_count$threshold, aes(yintercept = min), linetype = "dashed") + 
#   geom_hline(data = cell_count$threshold, aes(yintercept = max), linetype = "dashed") + 
#   facet_grid(run ~ subsection) + 
#   facet_grid(run ~ section) + 
#   
#   theme(axis.text.x = element_blank(),
#         legend.position = "right") + 
#   scale_color_brewer(type="qual", palette = 2) +
#   labs(title = "Number of objects and their distribution across the plate",
#        subtitle = "shown by sections",
#        caption = "") +
#   ggsave(file = here::here(paste("figures/number_of_objects_distribution_by_section_barcode_", selected_barcode, ".png", sep="")))
```

```{r clustering based on intensity}
# dye_start_time = Sys.time()
# 
# dye_intensity <- observation_f %>% 
#   dplyr::select(intensity_integrated_intensity_ch3, intensity_integrated_intensity_ch4, 
#                 id_barcode, id_measurement, id_observation,
#                 time, well, compound_name, mmoles_per_liter, measurement_no, iteration_no,
#                 object_number, image_number, run) %>% 
#   collect() 
# 
# # dye_intensity <- dye_intensity %>% 
#   # mutate(run = paste0(measurement_no, "_", iteration_no)) %>%
#   # mutate(intensity_integrated_intensity_ch3_log = log(intensity_integrated_intensity_ch3)) %>% 
#   # mutate(intensity_integrated_intensity_ch4_log = log(intensity_integrated_intensity_ch4)) %>% 
#   # group_by(id_barcode, run) %>% 
#   # mutate(intensity_integrated_intensity_ch3_log_scale = scale(intensity_integrated_intensity_ch3_log)) %>% 
#   # mutate(intensity_integrated_intensity_ch4_log_scale = scale(intensity_integrated_intensity_ch4_log)) %>% 
#   # ungroup()
# 
# dye_cluster <- dye_intensity %>% 
#   select(intensity_integrated_intensity_ch4, # also test without log_scale in a bit
#          intensity_integrated_intensity_ch3,
#          id_barcode) %>% 
#   # sample_n(size = 50000) %>%
#   nest(contains('integrated')) %>% 
#   #mutate(kmeans = purrr::map(data, ~ .x %>% as.matrix %>% kmeans(centers = 2)),
#   mutate(kmeans = purrr::map(data, ~ .x %>% as.matrix %>% mclust::Mclust(., G = 2)),
#          cluster = purrr::map(kmeans, ~ .x %>% .$classification),
#          uncertainty = purrr::map(kmeans, ~ .x %>% .$uncertainty)) %>% 
#   unnest(data, cluster, uncertainty)
# 
# # Reordering cluster labels, so the cluster with the lower average ch4 intensity is labeled as cluster == "dead"
# 
# 
# dye_overview <- dye_cluster %>% 
#   group_by(id_barcode, cluster) %>% 
#   summarise(ch4_mean = mean(intensity_integrated_intensity_ch4)) %>%
#   arrange(id_barcode, ch4_mean) %>%
#   cbind(., new_cluster = rep(c(1,2), times = nrow(dye_cluster %>% 
#                                               count(id_barcode)))) %>% 
#   left_join(dye_cluster, .) %>% 
#   select(-cluster, -ch4_mean) %>%
#   rename(cluster = new_cluster) %>% 
#   mutate(cluster = if_else(cluster == 1, "dead", "alive") %>% factor) %>% 
#   cbind(., dye_intensity %>% 
#                         select(-id_barcode, 
#                                - contains('intensity_integrated_intensity_ch3'), 
#                                - contains('intensity_integrated_intensity_ch4')))
# 
# # dye_overview %>%
# #   mutate(cluster = if_else(cluster == 1, "viable", "dead")) %>%
# #   ggplot(aes(intensity_integrated_intensity_ch4_log, fill = cluster)) + 
# #   geom_histogram(position = "identity") +
# #   theme_bw() + 
# #   facet_grid(run ~ id_barcode) + 
# #   scale_x_log10() 
# 
# dye_end_time = Sys.time()
# print(dye_end_time - dye_start_time)

# dye_overview %>% write_rds(here::here("data/dye_overview.Rds"))
```

```{r}
# dye_overview <- read_rds(here::here("data/dye_overview.Rds"))

# dye_overview  %>%
#   #sample_n(100) %>%
#   # mutate(cluster = if_else(cluster == 1, "viable", "dead")) %>%
#   ggplot(aes(intensity_integrated_intensity_ch4, intensity_integrated_intensity_ch3)) +
#   # geom_hex() +
#   geom_point(aes(color=cluster))+
#   theme_bw() +
#   facet_grid(run ~ id_barcode) +
#   scale_x_log10() +
#   scale_y_log10() +
#   scale_fill_viridis_c() +
#   # geom_density_2d(aes(color = cluster)) +
#   scale_color_brewer(type = "qual") +
#   ggsave(here::here("figures/clustering.png"))
```

#TODOs
access clustering label from dye_overview object, cluster variable
calculate proportions of living cells per timepoint 

```{r}
save(list = c("selected_barcode", "plate_anno", "anno", "anno_w_time", "measurement", "measurement_layout", "observation_f"), file = here::here(paste("runtimes/vars_20200725_w_observation_f_raw_barcode_", selected_barcode, ".RData", sep="")))

# write.csv(observation_f, paste0("data/observations_barcode_", selected_barcode, ".csv"))

# save(list = c("selected_barcode", "plate_anno", "anno", "anno_w_time", "barcode_anno", "measurement", "measurement_layout", "observation_f"), file = here::here(paste("runtimes/vars_20200723_w_observation_f_raw_barcode_", selected_barcode, ".RData", sep="")))
```
